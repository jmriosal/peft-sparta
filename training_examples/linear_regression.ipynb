{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f4b2078-fb09-4ca8-8c0c-c5f205d3e2f6",
   "metadata": {},
   "source": [
    "## Pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f52a413-baa4-43d6-95d6-3c9cf021134b",
   "metadata": {},
   "source": [
    "Create a **linear model** with 5 input features and 1 output feature\n",
    "\n",
    "$y = m(x) = w \\, x + b = \\sum_{i=1}^{5} w_i \\, x_i + b$\n",
    "\n",
    "where: \n",
    "\n",
    "* $x = [x_1, x_2, x_3, x_4, x_5]$ is the input feature vector, and\n",
    "\n",
    "* $w = [w_1, w_2, w_3, w_4, w_5]$ and $b$ are the model parameters (weight and bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47ea60c7-351f-4d2c-80c3-2492e649aad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28f82953-63be-48e7-b937-4dd7720d96df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=5, out_features=1, bias=True)\n"
     ]
    }
   ],
   "source": [
    "model = nn.Linear(5,1)  \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd2c995-3c78-4867-b9d6-4c0262f263c5",
   "metadata": {},
   "source": [
    "Model parameters (*weights* and *bias*) at initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1efa115e-2818-4606-981a-0542cb896ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight: Parameter containing:\n",
      "tensor([[0.0034, 0.4410, 0.4370, 0.4056, 0.3660]], requires_grad=True)\n",
      "\n",
      "bias: Parameter containing:\n",
      "tensor([-0.1786], requires_grad=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n, p in model.named_parameters():\n",
    "    print(f\"{n}: {p}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef713f0-b389-477b-82cc-1bb38694b908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c128f74-c045-4710-813f-ce87eb11172e",
   "metadata": {},
   "source": [
    "We can test the model by computing its output $y = m(x)$, for $x=(1,1,1,1,1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "219ad171-1b23-4f65-b2c6-826982c1be24",
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = torch.tensor([1.,1.,1.,1.,1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b5818b0-6ef7-4728-a315-267ab9b6bc80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4744436740875244"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y = model(ones)\n",
    "y.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d275409e-f88e-4783-9a76-2da0375c0f33",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this case, for $x=(1,1,1,1,1)$, $y = m(x) = w_1 + w_2 + w_3 + w_4 + w_5 + b$. Let's check this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbf966df-1d15-41e4-b7a4-9e11469644a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4744436740875244"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model.weight.data.sum() + model.bias.data).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c30ad92-4efa-4959-b362-57faf49bfb34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0d5df4c-8cc0-40ef-b0e5-c05f9b5dd88a",
   "metadata": {},
   "source": [
    "## Create SpaRTA adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93032498-9846-4717-b740-a865dd7a78f7",
   "metadata": {},
   "source": [
    "We use the `SpaRTA` class to add a SpaRTA adapter to the model. \n",
    "\n",
    "By choosing a $sparsity \\approx 0.0$, the SpaRTA adapter will mark all the model parameters $(w_1, ...,w_5)$ and $b$ as trainable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8bc825e-5bbd-4827-bf42-efd3c67fe6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft_sparta import SpaRTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2c9bee8-bf94-4bf7-852a-ab8a4ab58646",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SpaRTA(model, 1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f78126-4e21-44c3-b4fa-9da50047fd63",
   "metadata": {},
   "source": [
    "Let's have a look into the internals of the created SpaRTA adapter (at initialization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e172731-158a-40c1-95c4-d6c318ddff78",
   "metadata": {},
   "source": [
    "* Non-trainable **indices** for each of the pre-trained model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96c83946-2aed-4f72-ba9a-07995182db3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight:\n",
      " tensor([[0, 0],\n",
      "        [0, 1],\n",
      "        [0, 2],\n",
      "        [0, 3],\n",
      "        [0, 4]], dtype=torch.int32)\n",
      "\n",
      "bias:\n",
      " tensor([[0]], dtype=torch.int32)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n, p in model.indices._buffers.items():\n",
    "    print(f\"{n}:\\n {p}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c29f278-8ad2-4ad5-ab4c-0c1b58627491",
   "metadata": {},
   "source": [
    "The **indices** (non-trainable) keep track of which specific model parameters are selected as trainable and therefore can be updated during training. Note here how all the model parameters have been selected as trainable  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a48356-32d1-4958-8c72-3a6b61e5667b",
   "metadata": {
    "tags": []
   },
   "source": [
    "* Trainable **deltas** (adapter parameters) for each of the pre-trained model parameters:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4682c32d-34f8-4f1c-a536-265e07ad335e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight: Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0.], requires_grad=True)\n",
      "\n",
      "bias: Parameter containing:\n",
      "tensor([0.], requires_grad=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n, p in model.deltas._parameters.items(): # model.named_parameters()\n",
    "    print(f\"{n}: {p}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d567f04b-f0fd-4195-b6a0-199d2991ca2d",
   "metadata": {},
   "source": [
    "The **deltas** (trainable) represent changes to the original model parameters on the given indices, and they will be added to the original model parameter values (that are kept frozen) at the beginning of each forward pass. Note how they are initialized at zero. Therefore, at initialization (before any training takes place), the adapted model has not changed and coincides with the original pre-trained model. We can test this by evaluating the model on $x=(1,1,1,1,1)$.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8c861c1-c9c6-4b03-aeb3-d83c3d598840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4744436740875244"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y = model(ones)\n",
    "y.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcdd9af-c1d7-4abe-be48-b942b709e43d",
   "metadata": {
    "tags": []
   },
   "source": [
    "* Now, the parameters of the pre-trained base model are frozen (non-trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f711431-17de-45a8-b328-29140e1edaa7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight: Parameter containing:\n",
      "tensor([[0.0034, 0.4410, 0.4370, 0.4056, 0.3660]])\n",
      "bias: Parameter containing:\n",
      "tensor([-0.1786])\n"
     ]
    }
   ],
   "source": [
    "# original (frozen) parameters of our pre-trained linear model\n",
    "for n, p in model.model.named_parameters():\n",
    "    print(f\"{n}: {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c34f6f-4bfd-41b1-91c2-d05c1e2698d3",
   "metadata": {},
   "source": [
    "* We also keep a copy of the original model parameters for the selected indices, so we can recover the model original parameter values (unmerging the deltas) after modifying them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5489505f-6d2f-42af-9cae-c09c734ca705",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's see all the trainable and non-trainable parameters of our SpaRTA adapter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84a1f4b3-fab5-4687-a274-a5c085c81745",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.weight:\n",
      " tensor([[0.0034, 0.4410, 0.4370, 0.4056, 0.3660]])\n",
      "\n",
      "model.bias:\n",
      " tensor([-0.1786])\n",
      "\n",
      "indices.weight:\n",
      " tensor([[0, 0],\n",
      "        [0, 1],\n",
      "        [0, 2],\n",
      "        [0, 3],\n",
      "        [0, 4]], dtype=torch.int32)\n",
      "\n",
      "indices.bias:\n",
      " tensor([[0]], dtype=torch.int32)\n",
      "\n",
      "deltas.weight:\n",
      " tensor([0., 0., 0., 0., 0.])\n",
      "\n",
      "deltas.bias:\n",
      " tensor([0.])\n",
      "\n",
      "original_chosen_params.weight:\n",
      " tensor([0.0034, 0.4410, 0.4370, 0.4056, 0.3660])\n",
      "\n",
      "original_chosen_params.bias:\n",
      " tensor([-0.1786])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# all trainable and non-trainable params (adapter state dict)\n",
    "for n, p in model.state_dict().items():\n",
    "    print(f\"{n}:\\n {p}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bad802b0-1876-4a2b-8b5f-ee7c2b87bb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained base model params:\n",
      " - weight\n",
      " - bias\n",
      "\n",
      "SpaRTA trainable (deltas) params:\n",
      " - deltas.weight\n",
      " - deltas.bias\n",
      "\n",
      "SpaRTA non-trainable (indices, original_chosen_params) params:\n",
      " - indices.weight\n",
      " - indices.bias\n",
      " - original_chosen_params.weight\n",
      " - original_chosen_params.bias\n"
     ]
    }
   ],
   "source": [
    "print('Pre-trained base model params:')\n",
    "for n, _ in model.model.named_parameters():\n",
    "    print(' -', n)\n",
    "\n",
    "print('\\nSpaRTA trainable (deltas) params:')\n",
    "for n, _ in model.named_parameters():\n",
    "    print(' -', n)\n",
    "\n",
    "print('\\nSpaRTA non-trainable (indices, original_chosen_params) params:')\n",
    "for n, _ in model.named_buffers():\n",
    "    print(' -', n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b73aad-a873-4098-9b19-98f8e51e2fab",
   "metadata": {},
   "source": [
    "Thus, for each of the original pre-trained model parameters, the SpaRTA adapter uses *indices* and *deltas* to represent which subset of scalar values within those parameter tensors can be modified (adapted) during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "311d4cb6-5aac-48c3-94da-0c2515f0d7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* model param: weight \n",
      "\n",
      "  - indices: tensor([[0, 0],\n",
      "        [0, 1],\n",
      "        [0, 2],\n",
      "        [0, 3],\n",
      "        [0, 4]], dtype=torch.int32) \n",
      "\n",
      "  - deltas: Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0.], requires_grad=True) \n",
      "\n",
      "* model param: bias \n",
      "\n",
      "  - indices: tensor([[0]], dtype=torch.int32) \n",
      "\n",
      "  - deltas: Parameter containing:\n",
      "tensor([0.], requires_grad=True) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for param_name, param in model.model.named_parameters():\n",
    "    print('* model param:', param_name, '\\n')\n",
    "\n",
    "    param_indices = model.indices[param_name]\n",
    "    print(\"  - indices:\", param_indices, '\\n')\n",
    "\n",
    "    param_deltas = model.deltas[param_name]\n",
    "    print(\"  - deltas:\", param_deltas, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e358f2c9-8c1e-4f5b-8046-bf89c045863d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94578211-74d1-4ed4-ae79-6b532fd8111e",
   "metadata": {},
   "source": [
    "## Training the SpaRTA adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bc3f24-91f4-496b-a049-2c347875a30a",
   "metadata": {},
   "source": [
    "We now train (using standard Stochastic Gradient Descent) the SpaRTA adapter (specifically its deltas) to fit the following dataset of $(x,y)$ pairs, where the target $y$ is simply the sum of the input features in $x$. That is, the parametric linear model to be leart (the one that generated the dataset) is one with parameters $w_1=w_2=...=w_5=1$ for its wights, and $b=0$ for its bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a4b3b3-79ab-4eb5-98fb-311f5b421e72",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ac795b5-1242-4ecd-8954-2a9ca07c7bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset generated from a linear model with \n",
    "# w1 = w2 = w3 = w4 = w5 = 1; b = 0\n",
    "x = torch.randn(50, 5)\n",
    "y = x.sum(-1) # y = x1 + x2 + x3 + x4 + x5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8b438d-6b8d-416a-9745-daa72c97bbf0",
   "metadata": {},
   "source": [
    "#### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffc9e5b6-f289-49d7-9e20-9d02240339ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1ca5bf1-c220-4dbc-9f8f-50cdae3927ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b92a069-0f2f-4786-8cdd-539ff6996c38",
   "metadata": {},
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8f3d27-5db9-46d3-b7c4-7987ece3e509",
   "metadata": {},
   "source": [
    "We use the Mean Squared Error (MSE) loss to measure the discrepancy between the targets and the model predictions in the following training loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2bddc23-d96e-49b4-ad64-81eef3c7231b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 60.549877;  Gradient norm: 82.7082\n",
      "Training Loss: 14.997356;  Gradient norm: 38.1405\n",
      "Training Loss:  4.755784;  Gradient norm: 21.2599\n",
      "Training Loss:  1.579162;  Gradient norm: 12.2680\n",
      "Training Loss:  0.530923;  Gradient norm:  7.1481\n",
      "Training Loss:  0.179143;  Gradient norm:  4.1777\n",
      "Training Loss:  0.060528;  Gradient norm:  2.4450\n",
      "Training Loss:  0.020467;  Gradient norm:  1.4324\n",
      "Training Loss:  0.006926;  Gradient norm:  0.8399\n",
      "Training Loss:  0.002345;  Gradient norm:  0.4930\n",
      "Training Loss:  0.000795;  Gradient norm:  0.2897\n",
      "Training Loss:  0.000270;  Gradient norm:  0.1704\n",
      "Training Loss:  0.000092;  Gradient norm:  0.1003\n",
      "Training Loss:  0.000031;  Gradient norm:  0.0591\n",
      "Training Loss:  0.000011;  Gradient norm:  0.0349\n",
      "Training Loss:  0.000004;  Gradient norm:  0.0206\n",
      "Training Loss:  0.000001;  Gradient norm:  0.0122\n",
      "Training Loss:  0.000000;  Gradient norm:  0.0072\n",
      "Training Loss:  0.000000;  Gradient norm:  0.0043\n",
      "Training Loss:  0.000000;  Gradient norm:  0.0025\n",
      "Training Loss:  0.000000;  Gradient norm:  0.0015\n",
      "Training Loss:  0.000000;  Gradient norm:  0.0009\n",
      "Training Loss:  0.000000;  Gradient norm:  0.0005\n",
      "Training Loss:  0.000000;  Gradient norm:  0.0003\n",
      "Training Loss:  0.000000;  Gradient norm:  0.0002\n",
      "Training Loss:  0.000000;  Gradient norm:  0.0001\n",
      "Training Loss:  0.000000;  Gradient norm:  0.0001\n",
      "Training Loss:  0.000000;  Gradient norm:  0.0000\n",
      "Training Loss:  0.000000;  Gradient norm:  0.0000\n",
      "Training Loss:  0.000000;  Gradient norm:  0.0000\n",
      "Training Loss:  0.000000;  Gradient norm:  0.0000\n",
      "Training Loss:  0.000000;  Gradient norm:  0.0000\n",
      "Training Loss:  0.000000;  Gradient norm:  0.0000\n",
      "Training Loss:  0.000000;  Gradient norm:  0.0000\n",
      "Training Loss:  0.000000;  Gradient norm:  0.0000\n",
      "Training Loss:  0.000000;  Gradient norm:  0.0000\n",
      "Training Loss:  0.000000;  Gradient norm:  0.0000\n",
      "Training Loss:  0.000000;  Gradient norm:  0.0000\n",
      "Training Loss:  0.000000;  Gradient norm:  0.0000\n",
      "Training Loss:  0.000000;  Gradient norm:  0.0000\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "for _ in range(40):\n",
    "\n",
    "    y_pred = model(x).squeeze()\n",
    "    loss = (y_pred - y).square().sum()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "        model.parameters(), \n",
    "        max_norm=float('inf'))\n",
    "\n",
    "    print(f'Training Loss: {loss.item():9.6f};  Gradient norm: {grad_norm.item():7.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12df4643-c77f-4e2c-ac31-163f9d25f840",
   "metadata": {
    "tags": []
   },
   "source": [
    "Here is the gradient of the adapter's delta parameters (close enough to zero, indicating convergance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33acd128-511c-4856-b6e8-d8234d23d20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4.7546e-07,  2.5568e-06, -1.0386e-06,  5.9224e-07, -1.2583e-06])\n",
      "tensor([-1.4901e-07])\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb87cba-0b35-4735-a5ed-4c418a68bde8",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f4f143-0900-4c31-a7ab-221da60263d7",
   "metadata": {},
   "source": [
    "Let's now check if the learnt model is actually the one that generated the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "377bbf7d-3275-4b9d-80aa-210fdeb7b0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa429a04-7177-4b9e-81c3-094412abcdc0",
   "metadata": {},
   "source": [
    "The final parameters (weights and bias) of the adapted model are the result of the original pre-trained model parameters (frozen) plus the learnt deltas. Calling the `eval` method on the adapter model will merge the deltas into the original pre-trained model parameters, if not already merged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a7a5596-46fa-43d8-ad77-6b0f54c89b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92e60367-47e1-484f-b445-e84d8d46e054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.9802e-08])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.bias.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d14a45f-39fd-459e-b7cc-c483d9bcb437",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3e915da-446f-439a-a936-5275b2906de8",
   "metadata": {},
   "source": [
    "Finally, we can also check this by evaluating the final adapted model in $x=(1,1,1,1,1)$. This should give us a value very close to $y = x_1 + x_2 + x_3 + x_4 + x_5 = 5.0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00406335-1e56-4843-b618-7707ef1c06a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aa3743-ebb0-470f-8980-3276e0761608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d3dd46-7579-4ee6-8f5d-f7c382360646",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
