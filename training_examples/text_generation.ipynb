{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff18e843-28d5-4ff2-a902-d72d2172ea51",
   "metadata": {},
   "source": [
    "### Fine-tuning with SpaRTA on a text-generation task "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33e5c64-b8c0-42cc-aa38-64a0b40b07f2",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78c044dc-0d00-45f3-8fdf-5146a6fa992e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73ccb48b-2361-47cc-9df4-fa41a3736d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-trained model \n",
    "model_id = 'Qwen/Qwen2.5-0.5B'\n",
    "\n",
    "# dir path for saving SpaRTA adapter\n",
    "home_dir = os.environ['HOME']\n",
    "save_dir = os.path.join(home_dir, 'sparta_examples/output/generative_model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2870a0f8-8a04-43a2-8cd4-ee39c6631e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf44bf0-0d5e-45ab-9aa5-f5337992334a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "369bd107-5a8b-4655-af38-d0e4f96f6ae0",
   "metadata": {},
   "source": [
    "#### Load pre-trained causal language model (for text-generation tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8200cb62-7bc5-43b4-b8b6-1380959593be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a56b33bd-a5cc-47e4-af7f-92a1553dc90a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90c24412ce4843d6aca43ddb78ab3dbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/290 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.truncation_side = 'right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4946de30-9d01-4764-8156-8b2fe05fbe4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55851d17-2b68-4f50-be4f-04ffe128f663",
   "metadata": {},
   "source": [
    "#### Create SpaRTA adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91efa833-4046-46d7-bed4-84042f530651",
   "metadata": {},
   "source": [
    "Instead of full fine-tuning (which is computationally expensive), we train a much smaller subset of model parameters using a SpaRTA adapter. We choose a `sparsity = 99%`, reducing the number of trainable parameters to just around 1% of the original model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9312460d-05ea-4336-a61f-0e2f8689506f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft_sparta import SpaRTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5404e4e5-7909-4fd4-bb6f-76a142438e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SpaRTA(model, 0.99).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987a05e7-99e8-4a54-9756-39b413b01869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c18b681-a946-4c5d-a26d-b4b4f5ca326c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num trainable parameters: 4,943,849 (1.00071%)\n"
     ]
    }
   ],
   "source": [
    "model.num_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5558f9ba-a681-43d8-90c7-d2cd33f3fc95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9da371d-eb38-4c38-bcf2-67c14a96895d",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78eb3b80-46ed-4868-8511-d2afd1a0f99c",
   "metadata": {},
   "source": [
    "We use `trl-lib/Capybara`, a conversational dataset for language modeling with demonstrations of multi-turn dialogues, to train our SpaRTA adapter so that the adapted base model can follow user instructions and engage in conversations (when using the chat template to format user/assistant interactions).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca04fe02-362e-4ed7-82c8-64b6295d26b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02eb4710-e5d2-4d38-86d8-cc324d00f09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset('trl-lib/Capybara', split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf38eee1-7fcf-4ee6-a461-120e322745e9",
   "metadata": {},
   "source": [
    "Let's see a training example in the dataset, which consists of multi-turn user/assistant chat interactions, using the helper `print_chat` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2604481c-6e81-4b66-83c0-03d7c5b10e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_chat(messages):\n",
    "    for i, msg in enumerate(messages):\n",
    "        role = msg['role']\n",
    "        content = msg['content']\n",
    "        print(f\"{i+1}. \\033[1m{role.upper()}:\\033[0m {content.strip()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e833490a-9643-4a29-a811-8112377ec531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. \u001b[1mUSER:\u001b[0m Determine the result obtained by evaluating 5338245-50629795848152. Numbers and symbols only, please.\n",
      "\n",
      "2. \u001b[1mASSISTANT:\u001b[0m 5338245 - 50629795848152 = -50629790509907\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# conversation example\n",
    "print_chat(train_dataset[1]['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1984131-276c-43eb-86c1-d761f689db35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00ff7a31-0b3e-4e41-a6e6-57303190d1e8",
   "metadata": {},
   "source": [
    "#### Train SpaRTA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b836448-09cc-4040-8c1e-6b015a8ca387",
   "metadata": {},
   "source": [
    "We use the `SFTTrainer` from the `trl` library to supervised fine-tune the adapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae03dc3-1627-4544-9cab-8a85ebdec005",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32e59bab-1645-4344-bb34-53295ba7efc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac76ed0e-ff16-49c1-85bb-f967e5b7bbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_config = SFTConfig(\n",
    "    output_dir=save_dir,\n",
    "    max_length=512,\n",
    "    num_train_epochs=1, # max_steps=100\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=1,\n",
    "    max_grad_norm=1.,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    logging_first_step=True,\n",
    "    gradient_checkpointing=False,\n",
    "    save_strategy=\"no\",\n",
    "    load_best_model_at_end=False,\n",
    "    report_to=\"none\",\n",
    "    push_to_hub=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a230151e-d7b5-4c07-ba0e-480e7bdc4496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4f9d0e27e0542a68b199c1c016c6ba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/15806 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    args=sft_config,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e170754c-888b-4407-8e6c-9dc33de9bdf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='988' max='988' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [988/988 05:13, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.072824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.687627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.663403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.614137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.619442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.610936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.609767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.602426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.602324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.612635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=988, training_loss=1.6218637413824135, metrics={'train_runtime': 314.7204, 'train_samples_per_second': 50.222, 'train_steps_per_second': 3.139, 'total_flos': 0.0, 'train_loss': 1.6218637413824135})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cffab9-c6ff-4246-aaba-cc0a48318383",
   "metadata": {},
   "source": [
    "Note how the training loss decreases from approximaltey 2.07 down to 1.61."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdc3361-3063-4395-a88e-da4ae6be7e3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4d4a37d-3373-4982-9d7a-7ce30b780c94",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6c8e26-326b-4f72-8341-513b6d36866b",
   "metadata": {},
   "source": [
    "Let's test the SpaRTA adapted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5765b08-7b07-4790-bd1e-fe07be17e0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "What is the capital of France?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "The capital of France is Paris. It is the largest city in Europe and the third-largest city in the world. It is also the seat of the French government and the country's cultural and political center\n"
     ]
    }
   ],
   "source": [
    "input_chat = [\n",
    "    {'content': 'What is the capital of France?', 'role': 'user'}\n",
    "]\n",
    "formatted_input = tokenizer.apply_chat_template(input_chat, tokenize=False, add_generation_prompt=True)\n",
    "tokenized_input = tokenizer(formatted_input, return_tensors=\"pt\").to(\"cuda\")\n",
    "model_response = model.generate(**tokenized_input, max_new_tokens=40, pad_token_id=tokenizer.pad_token_id)\n",
    "print(tokenizer.decode(model_response[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4f505ff-1fa7-426d-9ddc-b5ad68cb26e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris. It is the largest city in Europe and the third-largest city in the world. It is also the seat of the French government and the country's cultural and political center\n"
     ]
    }
   ],
   "source": [
    "# printout model response only\n",
    "input_length = tokenized_input.input_ids.shape[1]\n",
    "new_tokens = model_response[0][input_length:]\n",
    "print(tokenizer.decode(new_tokens, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732b3746-aa60-4126-88e3-a8502e6938d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "139574b8-daaa-4f7e-9202-1f419c26b4c3",
   "metadata": {},
   "source": [
    "#### Saving the SpaRTA adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa553ff6-8160-4955-accf-3c31d59c5664",
   "metadata": {},
   "source": [
    "We save the adapter using the `save` method with the argument `merged=False`, so only the adapter is saved, instead of the entire merged model, which will duplicate most of the parameters in the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198a6ce7-7337-47b7-92ae-6635f3ec9cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(save_dir, merged=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e29d42a-71e2-4aad-a3ce-9354bf3bf681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['config.json', 'sparse_deltas.safetensors']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4d5317-feeb-461a-b76e-79442e19cdf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1bb79d53-3c5f-4514-a8b1-05f7851dc306",
   "metadata": {},
   "source": [
    "#### Reloading the adapter for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b73b5737-a2d1-4fb3-8a7a-a17fa65c245d",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b9c694-c0fc-4b00-bb2d-3a941614fe46",
   "metadata": {},
   "source": [
    "We use the `SpaRTAforCausalLM` class to reload the SpaRTA adapter and marged it to the original pre-trained base model, `Qwen 2.5 0.5B`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10232186-80d6-4ccc-a861-859423086aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft_sparta import SpaRTAforCausalLM\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a2373d4-db1a-47e8-a864-aad1ae3959ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53c9ebfb6af24ff9a129f4989cd94cde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/290 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = SpaRTAforCausalLM(save_dir, 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e3ea53a-1570-4947-94ef-72ee8cdc32a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(SpaRTA)ModelForCausalLM(\n",
      "\tadapter = '/u/jriosal/sparta_examples/output/generative_model/'\n",
      "\tbase model = 'Qwen/Qwen2.5-0.5B'\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea100723-9b5c-4cdf-bebb-594c67a1f726",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model.base_model.config._name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e9f268d-d1aa-4163-b542-7dd87114fd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.padding_side = 'left'\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "56986da3-a7d3-41cd-a58e-8e6e30b8c92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# token ids used as stopping criteria during text generation\n",
    "gen_terminators = [\n",
    "    tokenizer.eos_token_id, \n",
    "    tokenizer.convert_tokens_to_ids('<|im_end|>')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1227fd9d-9844-48a9-add6-0f8cf5dd60a7",
   "metadata": {},
   "source": [
    "Let's test the reloaded adapted model in the same prompt as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "01beecbd-2510-4717-8269-bad43a31a56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "What is the capital of France?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "The capital of France is Paris. It is the largest city in Europe and the third-largest city in the world. It is also the seat of the French government and the country's cultural and political center\n"
     ]
    }
   ],
   "source": [
    "input_chat = [\n",
    "    {'content': 'What is the capital of France?', 'role': 'user'}\n",
    "]\n",
    "formatted_input = tokenizer.apply_chat_template(input_chat, tokenize=False, add_generation_prompt=True)\n",
    "tokenized_input = tokenizer(formatted_input, return_tensors=\"pt\").to(\"cuda\")\n",
    "model_response = model.generate(**tokenized_input, max_new_tokens=40, eos_token_id=gen_terminators)\n",
    "print(tokenizer.decode(model_response[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de4dee7-31b3-4840-833a-990ed502e7ba",
   "metadata": {},
   "source": [
    "The response of the reloaded adapted model is identical to the saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e79aab6-5274-4542-ba17-f309e913b798",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c0aa24-da56-4b0f-bc97-37634dde4490",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
